{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeakAlign demo notebook\n",
    "\n",
    "This notebook shows how to run a trained model on a given image pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "from os.path import exists\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.cnn_geometric_model import CNNGeometric, TwoStageCNNGeometric\n",
    "from data.pf_dataset import PFDataset\n",
    "from data.download_datasets import download_PF_pascal\n",
    "from image.normalization import NormalizeImageDict, normalize_image\n",
    "from util.torch_util import BatchTensorToVars, str_to_bool\n",
    "from geotnf.transformation import GeometricTnf\n",
    "from geotnf.point_tnf import *\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import warnings\n",
    "from torchvision.transforms import Normalize\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select one of the following models:\n",
    "# cnngeo_vgg16, cnngeo_resnet101, proposed_resnet101\n",
    "model_selection = 'proposed_resnet101' \n",
    "\n",
    "model_aff_path = ''\n",
    "model_tps_path = ''\n",
    "model_aff_tps_path = ''\n",
    "\n",
    "if model_selection=='cnngeo_vgg16':\n",
    "    model_aff_path = 'trained_models/trained_models/cnngeo_vgg16_affine.pth.tar'\n",
    "    model_tps_path = 'trained_models/trained_models/cnngeo_vgg16_tps.pth.tar'\n",
    "    feature_extraction_cnn = 'vgg'\n",
    "    \n",
    "elif model_selection=='cnngeo_resnet101':\n",
    "    model_aff_path = 'trained_models/trained_models/cnngeo_resnet101_affine.pth.tar'\n",
    "    model_tps_path = 'trained_models/trained_models/cnngeo_resnet101_tps.pth.tar'   \n",
    "    feature_extraction_cnn = 'resnet101'\n",
    "    \n",
    "elif model_selection=='proposed_resnet101':\n",
    "    model_aff_tps_path = 'trained_models/weakalign_resnet101_affine_tps.pth.tar'\n",
    "    feature_extraction_cnn = 'resnet101'\n",
    "    \n",
    "\n",
    "source_image_path='datasets/proposal-flow-pascal/PF-dataset-PASCAL/JPEGImages/2008_006325.jpg'\n",
    "target_image_path='datasets/proposal-flow-pascal/PF-dataset-PASCAL/JPEGImages/2010_004954.jpg'\n",
    "\n",
    "if not exists(source_image_path):\n",
    "    download_PF_pascal('datasets/proposal-flow-pascal/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "model = TwoStageCNNGeometric(use_cuda=use_cuda,\n",
    "                             return_correlation=False,\n",
    "                             feature_extraction_cnn=feature_extraction_cnn)\n",
    "\n",
    "# load pre-trained model\n",
    "if model_aff_tps_path!='':\n",
    "    checkpoint = torch.load(model_aff_tps_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "        \n",
    "    for name, param in model.FeatureExtraction.state_dict().items():\n",
    "        model.FeatureExtraction.state_dict()[name].copy_(checkpoint['state_dict']['FeatureExtraction.' + name])    \n",
    "    for name, param in model.FeatureRegression.state_dict().items():\n",
    "        model.FeatureRegression.state_dict()[name].copy_(checkpoint['state_dict']['FeatureRegression.' + name])\n",
    "    for name, param in model.FeatureRegression2.state_dict().items():\n",
    "        model.FeatureRegression2.state_dict()[name].copy_(checkpoint['state_dict']['FeatureRegression2.' + name])    \n",
    "else:\n",
    "    checkpoint_aff = torch.load(model_aff_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint_aff['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint_aff['state_dict'].items()])\n",
    "    for name, param in model.FeatureExtraction.state_dict().items():\n",
    "        model.FeatureExtraction.state_dict()[name].copy_(checkpoint_aff['state_dict']['FeatureExtraction.' + name])    \n",
    "    for name, param in model.FeatureRegression.state_dict().items():\n",
    "        model.FeatureRegression.state_dict()[name].copy_(checkpoint_aff['state_dict']['FeatureRegression.' + name])\n",
    "\n",
    "    checkpoint_tps = torch.load(model_tps_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint_tps['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint_tps['state_dict'].items()])\n",
    "    for name, param in model.FeatureRegression2.state_dict().items():\n",
    "        model.FeatureRegression2.state_dict()[name].copy_(checkpoint_tps['state_dict']['FeatureRegression.' + name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpsTnf = GeometricTnf(geometric_model='tps', use_cuda=use_cuda)\n",
    "affTnf = GeometricTnf(geometric_model='affine', use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resizeCNN = GeometricTnf(out_h=240, out_w=240, use_cuda = False) \n",
    "normalizeTnf = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # convert to torch Variable\n",
    "    image = np.expand_dims(image.transpose((2,0,1)),0)\n",
    "    image = torch.Tensor(image.astype(np.float32)/255.0)\n",
    "    image_var = Variable(image,requires_grad=False)\n",
    "\n",
    "    # Resize image using bilinear sampling with identity affine tnf\n",
    "    image_var = resizeCNN(image_var)\n",
    "    \n",
    "    # Normalize image\n",
    "    image_var = normalize_image(image_var)\n",
    "    \n",
    "    return image_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_image = io.imread(source_image_path)\n",
    "target_image = io.imread(target_image_path)\n",
    "\n",
    "source_image_var = preprocess_image(source_image)\n",
    "target_image_var = preprocess_image(target_image)\n",
    "\n",
    "if use_cuda:\n",
    "    source_image_var = source_image_var.cuda()\n",
    "    target_image_var = target_image_var.cuda()\n",
    "\n",
    "batch = {'source_image': source_image_var, 'target_image':target_image_var}\n",
    "\n",
    "resizeTgt = GeometricTnf(out_h=target_image.shape[0], out_w=target_image.shape[1], use_cuda = use_cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate model\n",
    "theta_aff,theta_aff_tps=model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute warped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affTpsTnf(source_image, theta_aff, theta_aff_tps, use_cuda=use_cuda):\n",
    "    tpstnf = GeometricTnf(geometric_model = 'tps',use_cuda=use_cuda)\n",
    "    sampling_grid = tpstnf(image_batch=source_image,\n",
    "                           theta_batch=theta_aff_tps,\n",
    "                           return_sampling_grid=True)[1]\n",
    "    X = sampling_grid[:,:,:,0].unsqueeze(3)\n",
    "    Y = sampling_grid[:,:,:,1].unsqueeze(3)\n",
    "    Xp = X*theta_aff[:,0].unsqueeze(1).unsqueeze(2)+Y*theta_aff[:,1].unsqueeze(1).unsqueeze(2)+theta_aff[:,2].unsqueeze(1).unsqueeze(2)\n",
    "    Yp = X*theta_aff[:,3].unsqueeze(1).unsqueeze(2)+Y*theta_aff[:,4].unsqueeze(1).unsqueeze(2)+theta_aff[:,5].unsqueeze(1).unsqueeze(2)\n",
    "    sg = torch.cat((Xp,Yp),3)\n",
    "    warped_image_batch = F.grid_sample(source_image, sg)\n",
    "\n",
    "    return warped_image_batch\n",
    "\n",
    "warped_image_aff = affTnf(batch['source_image'],theta_aff.view(-1,2,3))\n",
    "warped_image_aff_tps = affTpsTnf(batch['source_image'],theta_aff,theta_aff_tps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-normalize images and convert to numpy\n",
    "warped_image_aff_np = normalize_image(resizeTgt(warped_image_aff),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "warped_image_aff_tps_np = normalize_image(resizeTgt(warped_image_aff_tps),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "\n",
    "N_subplots = 4\n",
    "fig, axs = plt.subplots(1,N_subplots)\n",
    "axs[0].imshow(source_image)\n",
    "axs[0].set_title('src')\n",
    "axs[1].imshow(target_image)\n",
    "axs[1].set_title('tgt')\n",
    "axs[2].imshow(warped_image_aff_np)\n",
    "axs[2].set_title('aff')\n",
    "axs[3].imshow(warped_image_aff_tps_np)\n",
    "axs[3].set_title('aff+tps')\n",
    "\n",
    "for i in range(N_subplots):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "fig.set_dpi(150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
